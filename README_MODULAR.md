# GCP RAG - Modular Microservices Architecture

A production-grade Retrieval-Augmented Generation (RAG) system on Google Cloud Platform, split into 4 independently deployable microservices.

## ğŸ—ï¸ Architecture

```
Frontend (8003)
    â†“
Synthesis (8002) â†’ Retrieval (8001)
    â†“                  â†“
    â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Pinecone
           â†‘
        (upload)
           â†‘
     Ingestion (8000)
```

**Services**:
- **Ingestion** (Port 8000): PDF upload â†’ chunking â†’ embedding â†’ Pinecone storage
- **Retrieval** (Port 8001): Query embedding â†’ vector search â†’ ranking â†’ chunks
- **Synthesis** (Port 8002): Calls Retrieval + LLM â†’ response with citations
- **Frontend** (Port 8003): SSE streaming API consumer

## ğŸš€ Quick Start

### Prerequisites
```bash
export OPENAI_API_KEY=sk-...
export PINECONE_API_KEY=pk-...
export GCP_PROJECT_ID=your-project
export GCS_BUCKET_NAME=your-bucket
```

### Run All Services (4 terminals)

**Terminal 1 - Ingestion**:
```bash
cd apps/ingestion && pip install -r requirements.txt && cp .env.example .env
# Edit .env with your keys
uvicorn app:app --reload --port 8000
```

**Terminal 2 - Retrieval**:
```bash
cd apps/retrieval && pip install -r requirements.txt && cp .env.example .env
uvicorn app:app --reload --port 8001
```

**Terminal 3 - Synthesis**:
```bash
cd apps/synthesis && pip install -r requirements.txt && cp .env.example .env
# Set RETRIEVAL_SERVICE_URL=http://localhost:8001 in .env
uvicorn app:app --reload --port 8002
```

**Terminal 4 - Frontend**:
```bash
cd apps/frontend && pip install -r requirements.txt && cp .env.example .env
# Set SYNTHESIS_SERVICE_URL=http://localhost:8002 in .env
uvicorn app:app --reload --port 8003
```

### Test the System
```bash
# Check health
curl http://localhost:8000/api/v1/health
curl http://localhost:8001/api/v1/health
curl http://localhost:8002/api/v1/health
curl http://localhost:8003/api/v1/health

# Ingest a PDF
curl -X POST -F "file=@/path/to/document.pdf" http://localhost:8000/api/v1/ingest

# Query
curl -X POST -H "Content-Type: application/json" \
  -d '{"query":"What is the main topic?"}' \
  http://localhost:8003/api/v1/query
```

## ğŸ“‹ Project Structure

```
gcp-rag/
â”œâ”€â”€ common/                    # Shared library (models, config, metrics)
â”‚   â”œâ”€â”€ models.py             # Pydantic models for all services
â”‚   â”œâ”€â”€ config.py             # Settings with lru_cache
â”‚   â”œâ”€â”€ metrics.py            # Metrics collection and timing
â”‚   â””â”€â”€ utils.py              # Utilities (chunking, cost estimation)
â”‚
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ ingestion/            # PDF ingestion service
â”‚   â”‚   â”œâ”€â”€ app.py            # FastAPI entry point
â”‚   â”‚   â”œâ”€â”€ config.py         # Ingestion settings
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ pipeline.py   # PDFExtractor, Embedding, VectorStore
â”‚   â”‚   â”œâ”€â”€ handlers/
â”‚   â”‚   â”‚   â””â”€â”€ routes.py     # POST /ingest, GET /health
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”‚   â”œâ”€â”€ conftest.py   # Pytest fixtures
â”‚   â”‚   â”‚   â””â”€â”€ test_pipeline.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ .env.example
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/            # Vector search service
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ pipeline.py   # Search, Ranking, Deduplication
â”‚   â”‚   â”œâ”€â”€ handlers/
â”‚   â”‚   â”‚   â””â”€â”€ routes.py     # POST /retrieve
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ .env.example
â”‚   â”‚
â”‚   â”œâ”€â”€ synthesis/            # LLM response generation
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ pipeline.py   # Calls Retrieval + LLM + Citations
â”‚   â”‚   â”œâ”€â”€ handlers/
â”‚   â”‚   â”‚   â””â”€â”€ routes.py     # POST /synthesize
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ .env.example
â”‚   â”‚
â”‚   â””â”€â”€ frontend/             # SSE streaming API
â”‚       â”œâ”€â”€ app.py
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ handlers/
â”‚       â”‚   â””â”€â”€ routes.py     # POST /query (SSE)
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ .env.example
â”‚
â”œâ”€â”€ terraform/
â”‚   â””â”€â”€ main.tf               # Cloud Run + Networking
â”‚
â”œâ”€â”€ ARCHITECTURE.md           # Detailed architecture
â””â”€â”€ README.md                 # This file
```

## ğŸ§ª Testing

Each service has comprehensive unit tests with mocked dependencies:

```bash
# Test all services
cd apps/ingestion && pytest tests/ -v
cd apps/retrieval && pytest tests/ -v
cd apps/synthesis && pytest tests/ -v
```

**Mock fixtures** (in `conftest.py`):
- âœ… OpenAI API mocked
- âœ… Pinecone index mocked
- âœ… httpx for inter-service calls mocked
- âœ… All tests run without API keys

## ğŸ”§ Configuration

All services use environment variables (see `.env.example` in each directory):

**Common**:
- `OPENAI_API_KEY` - OpenAI API key
- `PINECONE_API_KEY` - Pinecone API key
- `GCP_PROJECT_ID` - GCP project ID
- `GCS_BUCKET_NAME` - GCS bucket name

**Ingestion**:
- `EMBEDDING_BATCH_SIZE` - Batch size for embeddings (default: 20)

**Retrieval**:
- `QUERY_TOP_K` - Number of chunks to retrieve (default: 10)

**Synthesis**:
- `RETRIEVAL_SERVICE_URL` - Retrieval service URL
- `LLM_TEMPERATURE` - LLM temperature (default: 0.7)
- `LLM_MAX_TOKENS` - Max tokens for response (default: 1000)
- `MAX_CONTEXT_TOKENS` - Max context tokens (default: 2000)

**Frontend**:
- `SYNTHESIS_SERVICE_URL` - Synthesis service URL
- `STREAMING_BUFFER_SIZE` - SSE buffer size (default: 100)

## ğŸ“¦ Deployment on GCP

### 1. Build Docker Images
```bash
export PROJECT_ID=your-project-id

for service in ingestion retrieval synthesis frontend; do
  docker build -t gcr.io/${PROJECT_ID}/rag-${service}:latest apps/${service}/
  docker push gcr.io/${PROJECT_ID}/rag-${service}:latest
done
```

### 2. Deploy with Terraform
```bash
cd terraform

# Create configuration
cat > terraform.tfvars <<EOF
gcp_project_id      = "your-project-id"
gcp_region          = "us-central1"
openai_api_key      = "sk-..."
pinecone_api_key    = "pk-..."
gcs_bucket_name     = "your-unique-bucket-name"
EOF

# Deploy
terraform init
terraform plan
terraform apply

# Get output URLs
terraform output frontend_url
```

## ğŸ“Š Monitoring

All services log metrics as JSON:

```json
{
  "query_id": "abc-123",
  "service": "synthesis",
  "latency_ms": 1250,
  "success": true,
  "tokens_used": 350,
  "cost_estimate": 0.015
}
```

View logs:
```bash
gcloud logging read "service:rag-*" --limit 50 --format=json
```

## ğŸ”— API Endpoints

### Ingestion Service
- `POST /api/v1/ingest` - Upload and ingest PDF
- `GET /api/v1/health` - Health check

### Retrieval Service
- `POST /api/v1/retrieve` - Search and retrieve chunks
- `GET /api/v1/health` - Health check

### Synthesis Service
- `POST /api/v1/synthesize` - Generate response with citations
- `GET /api/v1/health` - Health check

### Frontend Service
- `POST /api/v1/query` - Query with SSE streaming
- `GET /api/v1/health` - Health check

## âœ¨ Key Features

âœ… **Independent Services**: Deploy each service separately
âœ… **Type-Safe**: Pydantic models for all service boundaries
âœ… **Testable**: All dependencies mockable with pytest fixtures
âœ… **Scalable**: Each service auto-scales on Cloud Run
âœ… **Modular**: Clear separation of concerns
âœ… **Cost-Efficient**: Pay only for what you use
âœ… **Extensible**: Easy to add new services or features
âœ… **Monitored**: JSON logging for easy parsing

## ğŸš¦ What's Next

1. **Local Testing**: Start all 4 services and test end-to-end
2. **GCP Setup**: Create GCP project and enable services
3. **Deploy**: Build images and deploy with Terraform
4. **Monitor**: Watch logs and metrics in Cloud Console
5. **Scale**: Adjust Cloud Run min/max instances as needed

## ğŸ“š Documentation

- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Detailed architecture and design decisions
- **[DEVELOPMENT.md](DEVELOPMENT.md)** - Development guide (if exists)
- **[SETUP_CHECKLIST.md](SETUP_CHECKLIST.md)** - Step-by-step setup checklist

## ğŸ¤ Contributing

To add a new service:
1. Create `apps/newservice/` with same structure
2. Add models to `common/models.py`
3. Add settings to `common/config.py`
4. Create FastAPI app in `newservice/app.py`
5. Add tests in `newservice/tests/`
6. Add Dockerfile and requirements.txt
7. Update Terraform for Cloud Run deployment

## ğŸ“ License

MIT

---

**Questions?** Check [ARCHITECTURE.md](ARCHITECTURE.md) or review example code in the services.
